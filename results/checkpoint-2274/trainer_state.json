{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2274,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013192612137203167,
      "grad_norm": 4.774919033050537,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.1185,
      "step": 10
    },
    {
      "epoch": 0.026385224274406333,
      "grad_norm": 4.0481414794921875,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.1164,
      "step": 20
    },
    {
      "epoch": 0.0395778364116095,
      "grad_norm": 6.396896839141846,
      "learning_rate": 3e-06,
      "loss": 1.1605,
      "step": 30
    },
    {
      "epoch": 0.052770448548812667,
      "grad_norm": 4.164666652679443,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.113,
      "step": 40
    },
    {
      "epoch": 0.06596306068601583,
      "grad_norm": 4.952052593231201,
      "learning_rate": 5e-06,
      "loss": 1.1276,
      "step": 50
    },
    {
      "epoch": 0.079155672823219,
      "grad_norm": 2.9794704914093018,
      "learning_rate": 6e-06,
      "loss": 1.1003,
      "step": 60
    },
    {
      "epoch": 0.09234828496042216,
      "grad_norm": 3.698364019393921,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.1296,
      "step": 70
    },
    {
      "epoch": 0.10554089709762533,
      "grad_norm": 5.2654290199279785,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1304,
      "step": 80
    },
    {
      "epoch": 0.11873350923482849,
      "grad_norm": 6.673818588256836,
      "learning_rate": 9e-06,
      "loss": 1.1144,
      "step": 90
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 4.961310386657715,
      "learning_rate": 1e-05,
      "loss": 1.0906,
      "step": 100
    },
    {
      "epoch": 0.14511873350923482,
      "grad_norm": 6.018001556396484,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0792,
      "step": 110
    },
    {
      "epoch": 0.158311345646438,
      "grad_norm": 8.471850395202637,
      "learning_rate": 1.2e-05,
      "loss": 1.1028,
      "step": 120
    },
    {
      "epoch": 0.17150395778364116,
      "grad_norm": 6.7678728103637695,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.105,
      "step": 130
    },
    {
      "epoch": 0.18469656992084432,
      "grad_norm": 4.957172870635986,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0928,
      "step": 140
    },
    {
      "epoch": 0.19788918205804748,
      "grad_norm": 5.170859336853027,
      "learning_rate": 1.5e-05,
      "loss": 1.1274,
      "step": 150
    },
    {
      "epoch": 0.21108179419525067,
      "grad_norm": 5.3827362060546875,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.0753,
      "step": 160
    },
    {
      "epoch": 0.22427440633245382,
      "grad_norm": 4.848232746124268,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.1123,
      "step": 170
    },
    {
      "epoch": 0.23746701846965698,
      "grad_norm": 2.6167654991149902,
      "learning_rate": 1.8e-05,
      "loss": 1.11,
      "step": 180
    },
    {
      "epoch": 0.25065963060686014,
      "grad_norm": 3.279914140701294,
      "learning_rate": 1.9e-05,
      "loss": 1.0629,
      "step": 190
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 3.0368456840515137,
      "learning_rate": 2e-05,
      "loss": 1.0691,
      "step": 200
    },
    {
      "epoch": 0.2770448548812665,
      "grad_norm": 3.081223726272583,
      "learning_rate": 2.1e-05,
      "loss": 1.0582,
      "step": 210
    },
    {
      "epoch": 0.29023746701846964,
      "grad_norm": 6.3455424308776855,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.0174,
      "step": 220
    },
    {
      "epoch": 0.3034300791556728,
      "grad_norm": 5.767125606536865,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9924,
      "step": 230
    },
    {
      "epoch": 0.316622691292876,
      "grad_norm": 4.938936233520508,
      "learning_rate": 2.4e-05,
      "loss": 1.0158,
      "step": 240
    },
    {
      "epoch": 0.32981530343007914,
      "grad_norm": 6.259737968444824,
      "learning_rate": 2.5e-05,
      "loss": 0.9764,
      "step": 250
    },
    {
      "epoch": 0.34300791556728233,
      "grad_norm": 6.126026630401611,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.99,
      "step": 260
    },
    {
      "epoch": 0.3562005277044855,
      "grad_norm": 3.945953845977783,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.0081,
      "step": 270
    },
    {
      "epoch": 0.36939313984168864,
      "grad_norm": 8.928479194641113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9792,
      "step": 280
    },
    {
      "epoch": 0.38258575197889183,
      "grad_norm": 6.918982028961182,
      "learning_rate": 2.9e-05,
      "loss": 0.9981,
      "step": 290
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 4.9141459465026855,
      "learning_rate": 3e-05,
      "loss": 0.9905,
      "step": 300
    },
    {
      "epoch": 0.40897097625329815,
      "grad_norm": 6.624162673950195,
      "learning_rate": 3.1e-05,
      "loss": 0.9372,
      "step": 310
    },
    {
      "epoch": 0.42216358839050133,
      "grad_norm": 6.654886245727539,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.0434,
      "step": 320
    },
    {
      "epoch": 0.43535620052770446,
      "grad_norm": 5.468522071838379,
      "learning_rate": 3.3e-05,
      "loss": 0.9974,
      "step": 330
    },
    {
      "epoch": 0.44854881266490765,
      "grad_norm": 4.315779209136963,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.949,
      "step": 340
    },
    {
      "epoch": 0.46174142480211083,
      "grad_norm": 6.919183254241943,
      "learning_rate": 3.5e-05,
      "loss": 1.0218,
      "step": 350
    },
    {
      "epoch": 0.47493403693931396,
      "grad_norm": 6.891890525817871,
      "learning_rate": 3.6e-05,
      "loss": 0.9498,
      "step": 360
    },
    {
      "epoch": 0.48812664907651715,
      "grad_norm": 6.477381229400635,
      "learning_rate": 3.7e-05,
      "loss": 0.9421,
      "step": 370
    },
    {
      "epoch": 0.5013192612137203,
      "grad_norm": 6.066005706787109,
      "learning_rate": 3.8e-05,
      "loss": 0.975,
      "step": 380
    },
    {
      "epoch": 0.5145118733509235,
      "grad_norm": 8.230584144592285,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.913,
      "step": 390
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 7.7161970138549805,
      "learning_rate": 4e-05,
      "loss": 0.9692,
      "step": 400
    },
    {
      "epoch": 0.5408970976253298,
      "grad_norm": 8.612902641296387,
      "learning_rate": 4.1e-05,
      "loss": 0.973,
      "step": 410
    },
    {
      "epoch": 0.554089709762533,
      "grad_norm": 9.606378555297852,
      "learning_rate": 4.2e-05,
      "loss": 0.9471,
      "step": 420
    },
    {
      "epoch": 0.5672823218997362,
      "grad_norm": 5.748153209686279,
      "learning_rate": 4.3e-05,
      "loss": 0.911,
      "step": 430
    },
    {
      "epoch": 0.5804749340369393,
      "grad_norm": 4.090378761291504,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.9028,
      "step": 440
    },
    {
      "epoch": 0.5936675461741425,
      "grad_norm": 6.769626617431641,
      "learning_rate": 4.5e-05,
      "loss": 0.8971,
      "step": 450
    },
    {
      "epoch": 0.6068601583113457,
      "grad_norm": 7.566318988800049,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.9425,
      "step": 460
    },
    {
      "epoch": 0.6200527704485488,
      "grad_norm": 10.706098556518555,
      "learning_rate": 4.7e-05,
      "loss": 0.9261,
      "step": 470
    },
    {
      "epoch": 0.633245382585752,
      "grad_norm": 5.926129341125488,
      "learning_rate": 4.8e-05,
      "loss": 0.9679,
      "step": 480
    },
    {
      "epoch": 0.6464379947229552,
      "grad_norm": 8.448880195617676,
      "learning_rate": 4.9e-05,
      "loss": 0.9818,
      "step": 490
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 5.845765590667725,
      "learning_rate": 5e-05,
      "loss": 0.9558,
      "step": 500
    },
    {
      "epoch": 0.6728232189973615,
      "grad_norm": 6.446487903594971,
      "learning_rate": 4.971815107102593e-05,
      "loss": 0.9924,
      "step": 510
    },
    {
      "epoch": 0.6860158311345647,
      "grad_norm": 7.992798805236816,
      "learning_rate": 4.943630214205186e-05,
      "loss": 0.8883,
      "step": 520
    },
    {
      "epoch": 0.6992084432717678,
      "grad_norm": 7.444221496582031,
      "learning_rate": 4.9154453213077795e-05,
      "loss": 1.0339,
      "step": 530
    },
    {
      "epoch": 0.712401055408971,
      "grad_norm": 8.335979461669922,
      "learning_rate": 4.887260428410372e-05,
      "loss": 0.9331,
      "step": 540
    },
    {
      "epoch": 0.7255936675461742,
      "grad_norm": 6.685624122619629,
      "learning_rate": 4.8590755355129655e-05,
      "loss": 0.8802,
      "step": 550
    },
    {
      "epoch": 0.7387862796833773,
      "grad_norm": 4.69497537612915,
      "learning_rate": 4.830890642615558e-05,
      "loss": 0.9464,
      "step": 560
    },
    {
      "epoch": 0.7519788918205804,
      "grad_norm": 6.663929462432861,
      "learning_rate": 4.8027057497181515e-05,
      "loss": 0.9533,
      "step": 570
    },
    {
      "epoch": 0.7651715039577837,
      "grad_norm": 6.173933982849121,
      "learning_rate": 4.774520856820744e-05,
      "loss": 0.8928,
      "step": 580
    },
    {
      "epoch": 0.7783641160949868,
      "grad_norm": 8.124838829040527,
      "learning_rate": 4.7463359639233375e-05,
      "loss": 0.9052,
      "step": 590
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 8.628262519836426,
      "learning_rate": 4.71815107102593e-05,
      "loss": 0.8878,
      "step": 600
    },
    {
      "epoch": 0.8047493403693932,
      "grad_norm": 10.564031600952148,
      "learning_rate": 4.6899661781285234e-05,
      "loss": 0.9149,
      "step": 610
    },
    {
      "epoch": 0.8179419525065963,
      "grad_norm": 10.276151657104492,
      "learning_rate": 4.661781285231117e-05,
      "loss": 0.8828,
      "step": 620
    },
    {
      "epoch": 0.8311345646437994,
      "grad_norm": 9.93553638458252,
      "learning_rate": 4.6335963923337094e-05,
      "loss": 0.8724,
      "step": 630
    },
    {
      "epoch": 0.8443271767810027,
      "grad_norm": 11.441752433776855,
      "learning_rate": 4.605411499436302e-05,
      "loss": 0.8671,
      "step": 640
    },
    {
      "epoch": 0.8575197889182058,
      "grad_norm": 5.396047592163086,
      "learning_rate": 4.5772266065388954e-05,
      "loss": 0.9669,
      "step": 650
    },
    {
      "epoch": 0.8707124010554089,
      "grad_norm": 4.161830425262451,
      "learning_rate": 4.549041713641488e-05,
      "loss": 0.8604,
      "step": 660
    },
    {
      "epoch": 0.8839050131926122,
      "grad_norm": 7.879155158996582,
      "learning_rate": 4.5208568207440814e-05,
      "loss": 0.9596,
      "step": 670
    },
    {
      "epoch": 0.8970976253298153,
      "grad_norm": 5.262300968170166,
      "learning_rate": 4.492671927846675e-05,
      "loss": 0.9154,
      "step": 680
    },
    {
      "epoch": 0.9102902374670184,
      "grad_norm": 7.873614311218262,
      "learning_rate": 4.464487034949267e-05,
      "loss": 0.9494,
      "step": 690
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 11.614933013916016,
      "learning_rate": 4.43630214205186e-05,
      "loss": 0.9578,
      "step": 700
    },
    {
      "epoch": 0.9366754617414248,
      "grad_norm": 3.8953700065612793,
      "learning_rate": 4.408117249154454e-05,
      "loss": 0.9574,
      "step": 710
    },
    {
      "epoch": 0.9498680738786279,
      "grad_norm": 7.75072717666626,
      "learning_rate": 4.3799323562570466e-05,
      "loss": 0.9168,
      "step": 720
    },
    {
      "epoch": 0.9630606860158312,
      "grad_norm": 6.092296123504639,
      "learning_rate": 4.351747463359639e-05,
      "loss": 0.8972,
      "step": 730
    },
    {
      "epoch": 0.9762532981530343,
      "grad_norm": 5.517991542816162,
      "learning_rate": 4.3235625704622326e-05,
      "loss": 0.8938,
      "step": 740
    },
    {
      "epoch": 0.9894459102902374,
      "grad_norm": 7.129178047180176,
      "learning_rate": 4.295377677564825e-05,
      "loss": 0.9601,
      "step": 750
    },
    {
      "epoch": 1.0026385224274406,
      "grad_norm": 6.4660725593566895,
      "learning_rate": 4.2671927846674186e-05,
      "loss": 0.9424,
      "step": 760
    },
    {
      "epoch": 1.0158311345646438,
      "grad_norm": 5.839738368988037,
      "learning_rate": 4.239007891770011e-05,
      "loss": 0.7598,
      "step": 770
    },
    {
      "epoch": 1.029023746701847,
      "grad_norm": 11.204663276672363,
      "learning_rate": 4.2108229988726046e-05,
      "loss": 0.7973,
      "step": 780
    },
    {
      "epoch": 1.04221635883905,
      "grad_norm": 13.899859428405762,
      "learning_rate": 4.182638105975197e-05,
      "loss": 0.7154,
      "step": 790
    },
    {
      "epoch": 1.0554089709762533,
      "grad_norm": 4.370832920074463,
      "learning_rate": 4.1544532130777905e-05,
      "loss": 0.8394,
      "step": 800
    },
    {
      "epoch": 1.0686015831134565,
      "grad_norm": 8.75367259979248,
      "learning_rate": 4.126268320180384e-05,
      "loss": 0.7298,
      "step": 810
    },
    {
      "epoch": 1.0817941952506596,
      "grad_norm": 13.466239929199219,
      "learning_rate": 4.0980834272829765e-05,
      "loss": 0.693,
      "step": 820
    },
    {
      "epoch": 1.0949868073878628,
      "grad_norm": 8.089147567749023,
      "learning_rate": 4.069898534385569e-05,
      "loss": 0.7598,
      "step": 830
    },
    {
      "epoch": 1.108179419525066,
      "grad_norm": 9.85766887664795,
      "learning_rate": 4.0417136414881625e-05,
      "loss": 0.7899,
      "step": 840
    },
    {
      "epoch": 1.121372031662269,
      "grad_norm": 8.501721382141113,
      "learning_rate": 4.013528748590756e-05,
      "loss": 0.7725,
      "step": 850
    },
    {
      "epoch": 1.1345646437994723,
      "grad_norm": 4.6986565589904785,
      "learning_rate": 3.9853438556933485e-05,
      "loss": 0.7152,
      "step": 860
    },
    {
      "epoch": 1.1477572559366755,
      "grad_norm": 12.170966148376465,
      "learning_rate": 3.957158962795942e-05,
      "loss": 0.6843,
      "step": 870
    },
    {
      "epoch": 1.1609498680738786,
      "grad_norm": 13.197447776794434,
      "learning_rate": 3.9289740698985344e-05,
      "loss": 0.8539,
      "step": 880
    },
    {
      "epoch": 1.1741424802110818,
      "grad_norm": 10.160172462463379,
      "learning_rate": 3.900789177001127e-05,
      "loss": 0.7557,
      "step": 890
    },
    {
      "epoch": 1.187335092348285,
      "grad_norm": 18.181018829345703,
      "learning_rate": 3.872604284103721e-05,
      "loss": 0.6868,
      "step": 900
    },
    {
      "epoch": 1.200527704485488,
      "grad_norm": 7.7242751121521,
      "learning_rate": 3.844419391206314e-05,
      "loss": 0.8233,
      "step": 910
    },
    {
      "epoch": 1.2137203166226913,
      "grad_norm": 11.453360557556152,
      "learning_rate": 3.8162344983089064e-05,
      "loss": 0.7884,
      "step": 920
    },
    {
      "epoch": 1.2269129287598945,
      "grad_norm": 8.255464553833008,
      "learning_rate": 3.7880496054115e-05,
      "loss": 0.7526,
      "step": 930
    },
    {
      "epoch": 1.2401055408970976,
      "grad_norm": 5.805210113525391,
      "learning_rate": 3.759864712514093e-05,
      "loss": 0.781,
      "step": 940
    },
    {
      "epoch": 1.2532981530343008,
      "grad_norm": 4.730753421783447,
      "learning_rate": 3.731679819616686e-05,
      "loss": 0.752,
      "step": 950
    },
    {
      "epoch": 1.266490765171504,
      "grad_norm": 14.113203048706055,
      "learning_rate": 3.703494926719279e-05,
      "loss": 0.788,
      "step": 960
    },
    {
      "epoch": 1.279683377308707,
      "grad_norm": 11.156713485717773,
      "learning_rate": 3.6753100338218716e-05,
      "loss": 0.6835,
      "step": 970
    },
    {
      "epoch": 1.2928759894459103,
      "grad_norm": 6.789685249328613,
      "learning_rate": 3.647125140924464e-05,
      "loss": 0.8196,
      "step": 980
    },
    {
      "epoch": 1.3060686015831133,
      "grad_norm": 5.612956523895264,
      "learning_rate": 3.6189402480270576e-05,
      "loss": 0.8046,
      "step": 990
    },
    {
      "epoch": 1.3192612137203166,
      "grad_norm": 8.131961822509766,
      "learning_rate": 3.590755355129651e-05,
      "loss": 0.7727,
      "step": 1000
    },
    {
      "epoch": 1.3324538258575198,
      "grad_norm": 7.388738632202148,
      "learning_rate": 3.5625704622322436e-05,
      "loss": 0.8013,
      "step": 1010
    },
    {
      "epoch": 1.345646437994723,
      "grad_norm": 10.62366008758545,
      "learning_rate": 3.534385569334836e-05,
      "loss": 0.8025,
      "step": 1020
    },
    {
      "epoch": 1.358839050131926,
      "grad_norm": 6.736076831817627,
      "learning_rate": 3.5062006764374296e-05,
      "loss": 0.7658,
      "step": 1030
    },
    {
      "epoch": 1.3720316622691293,
      "grad_norm": 14.679655075073242,
      "learning_rate": 3.478015783540023e-05,
      "loss": 0.7765,
      "step": 1040
    },
    {
      "epoch": 1.3852242744063323,
      "grad_norm": 12.908125877380371,
      "learning_rate": 3.4498308906426155e-05,
      "loss": 0.7931,
      "step": 1050
    },
    {
      "epoch": 1.3984168865435356,
      "grad_norm": 5.549347400665283,
      "learning_rate": 3.421645997745209e-05,
      "loss": 0.826,
      "step": 1060
    },
    {
      "epoch": 1.4116094986807388,
      "grad_norm": 8.370938301086426,
      "learning_rate": 3.3934611048478015e-05,
      "loss": 0.7884,
      "step": 1070
    },
    {
      "epoch": 1.424802110817942,
      "grad_norm": 6.76110315322876,
      "learning_rate": 3.365276211950395e-05,
      "loss": 0.7371,
      "step": 1080
    },
    {
      "epoch": 1.437994722955145,
      "grad_norm": 10.686677932739258,
      "learning_rate": 3.337091319052988e-05,
      "loss": 0.7692,
      "step": 1090
    },
    {
      "epoch": 1.4511873350923483,
      "grad_norm": 9.013206481933594,
      "learning_rate": 3.308906426155581e-05,
      "loss": 0.8472,
      "step": 1100
    },
    {
      "epoch": 1.4643799472295513,
      "grad_norm": 8.006030082702637,
      "learning_rate": 3.2807215332581735e-05,
      "loss": 0.7936,
      "step": 1110
    },
    {
      "epoch": 1.4775725593667546,
      "grad_norm": 14.062169075012207,
      "learning_rate": 3.252536640360767e-05,
      "loss": 0.6907,
      "step": 1120
    },
    {
      "epoch": 1.4907651715039578,
      "grad_norm": 12.238818168640137,
      "learning_rate": 3.22435174746336e-05,
      "loss": 0.7271,
      "step": 1130
    },
    {
      "epoch": 1.503957783641161,
      "grad_norm": 4.583370208740234,
      "learning_rate": 3.196166854565953e-05,
      "loss": 0.8062,
      "step": 1140
    },
    {
      "epoch": 1.517150395778364,
      "grad_norm": 6.923147201538086,
      "learning_rate": 3.167981961668546e-05,
      "loss": 0.7033,
      "step": 1150
    },
    {
      "epoch": 1.5303430079155673,
      "grad_norm": 12.416332244873047,
      "learning_rate": 3.139797068771139e-05,
      "loss": 0.7674,
      "step": 1160
    },
    {
      "epoch": 1.5435356200527703,
      "grad_norm": 6.812841415405273,
      "learning_rate": 3.111612175873732e-05,
      "loss": 0.709,
      "step": 1170
    },
    {
      "epoch": 1.5567282321899736,
      "grad_norm": 12.708789825439453,
      "learning_rate": 3.083427282976325e-05,
      "loss": 0.7386,
      "step": 1180
    },
    {
      "epoch": 1.5699208443271768,
      "grad_norm": 6.587008476257324,
      "learning_rate": 3.055242390078918e-05,
      "loss": 0.7797,
      "step": 1190
    },
    {
      "epoch": 1.58311345646438,
      "grad_norm": 13.54847526550293,
      "learning_rate": 3.0270574971815107e-05,
      "loss": 0.746,
      "step": 1200
    },
    {
      "epoch": 1.596306068601583,
      "grad_norm": 5.945424556732178,
      "learning_rate": 2.9988726042841037e-05,
      "loss": 0.7024,
      "step": 1210
    },
    {
      "epoch": 1.6094986807387863,
      "grad_norm": 5.665104389190674,
      "learning_rate": 2.970687711386697e-05,
      "loss": 0.7996,
      "step": 1220
    },
    {
      "epoch": 1.6226912928759893,
      "grad_norm": 7.0391740798950195,
      "learning_rate": 2.94250281848929e-05,
      "loss": 0.757,
      "step": 1230
    },
    {
      "epoch": 1.6358839050131926,
      "grad_norm": 15.129524230957031,
      "learning_rate": 2.914317925591883e-05,
      "loss": 0.7721,
      "step": 1240
    },
    {
      "epoch": 1.6490765171503958,
      "grad_norm": 7.252932071685791,
      "learning_rate": 2.8861330326944756e-05,
      "loss": 0.774,
      "step": 1250
    },
    {
      "epoch": 1.662269129287599,
      "grad_norm": 8.50764274597168,
      "learning_rate": 2.8579481397970686e-05,
      "loss": 0.8067,
      "step": 1260
    },
    {
      "epoch": 1.675461741424802,
      "grad_norm": 6.990533351898193,
      "learning_rate": 2.829763246899662e-05,
      "loss": 0.7643,
      "step": 1270
    },
    {
      "epoch": 1.6886543535620053,
      "grad_norm": 13.39630126953125,
      "learning_rate": 2.801578354002255e-05,
      "loss": 0.8066,
      "step": 1280
    },
    {
      "epoch": 1.7018469656992083,
      "grad_norm": 10.75790786743164,
      "learning_rate": 2.773393461104848e-05,
      "loss": 0.8097,
      "step": 1290
    },
    {
      "epoch": 1.7150395778364116,
      "grad_norm": 8.505586624145508,
      "learning_rate": 2.745208568207441e-05,
      "loss": 0.6907,
      "step": 1300
    },
    {
      "epoch": 1.7282321899736148,
      "grad_norm": 11.688013076782227,
      "learning_rate": 2.7170236753100342e-05,
      "loss": 0.8379,
      "step": 1310
    },
    {
      "epoch": 1.741424802110818,
      "grad_norm": 4.244495868682861,
      "learning_rate": 2.6888387824126272e-05,
      "loss": 0.7032,
      "step": 1320
    },
    {
      "epoch": 1.754617414248021,
      "grad_norm": 5.4763617515563965,
      "learning_rate": 2.66065388951522e-05,
      "loss": 0.7035,
      "step": 1330
    },
    {
      "epoch": 1.767810026385224,
      "grad_norm": 9.331557273864746,
      "learning_rate": 2.632468996617813e-05,
      "loss": 0.7512,
      "step": 1340
    },
    {
      "epoch": 1.7810026385224274,
      "grad_norm": 7.604462623596191,
      "learning_rate": 2.604284103720406e-05,
      "loss": 0.7008,
      "step": 1350
    },
    {
      "epoch": 1.7941952506596306,
      "grad_norm": 13.975248336791992,
      "learning_rate": 2.576099210822999e-05,
      "loss": 0.7329,
      "step": 1360
    },
    {
      "epoch": 1.8073878627968338,
      "grad_norm": 8.140799522399902,
      "learning_rate": 2.547914317925592e-05,
      "loss": 0.7458,
      "step": 1370
    },
    {
      "epoch": 1.820580474934037,
      "grad_norm": 8.626213073730469,
      "learning_rate": 2.519729425028185e-05,
      "loss": 0.6874,
      "step": 1380
    },
    {
      "epoch": 1.83377308707124,
      "grad_norm": 3.844200611114502,
      "learning_rate": 2.491544532130778e-05,
      "loss": 0.7226,
      "step": 1390
    },
    {
      "epoch": 1.8469656992084431,
      "grad_norm": 9.6218843460083,
      "learning_rate": 2.463359639233371e-05,
      "loss": 0.7809,
      "step": 1400
    },
    {
      "epoch": 1.8601583113456464,
      "grad_norm": 7.94784688949585,
      "learning_rate": 2.435174746335964e-05,
      "loss": 0.8179,
      "step": 1410
    },
    {
      "epoch": 1.8733509234828496,
      "grad_norm": 10.888376235961914,
      "learning_rate": 2.406989853438557e-05,
      "loss": 0.6725,
      "step": 1420
    },
    {
      "epoch": 1.8865435356200528,
      "grad_norm": 4.975091457366943,
      "learning_rate": 2.37880496054115e-05,
      "loss": 0.7199,
      "step": 1430
    },
    {
      "epoch": 1.899736147757256,
      "grad_norm": 9.931329727172852,
      "learning_rate": 2.350620067643743e-05,
      "loss": 0.6571,
      "step": 1440
    },
    {
      "epoch": 1.912928759894459,
      "grad_norm": 9.12621021270752,
      "learning_rate": 2.322435174746336e-05,
      "loss": 0.6468,
      "step": 1450
    },
    {
      "epoch": 1.9261213720316621,
      "grad_norm": 9.160064697265625,
      "learning_rate": 2.2942502818489294e-05,
      "loss": 0.6989,
      "step": 1460
    },
    {
      "epoch": 1.9393139841688654,
      "grad_norm": 10.730857849121094,
      "learning_rate": 2.266065388951522e-05,
      "loss": 0.6603,
      "step": 1470
    },
    {
      "epoch": 1.9525065963060686,
      "grad_norm": 13.0791597366333,
      "learning_rate": 2.237880496054115e-05,
      "loss": 0.7734,
      "step": 1480
    },
    {
      "epoch": 1.9656992084432718,
      "grad_norm": 9.550217628479004,
      "learning_rate": 2.209695603156708e-05,
      "loss": 0.8576,
      "step": 1490
    },
    {
      "epoch": 1.978891820580475,
      "grad_norm": 8.91973876953125,
      "learning_rate": 2.181510710259301e-05,
      "loss": 0.6994,
      "step": 1500
    },
    {
      "epoch": 1.992084432717678,
      "grad_norm": 7.71386194229126,
      "learning_rate": 2.1533258173618943e-05,
      "loss": 0.7563,
      "step": 1510
    },
    {
      "epoch": 2.005277044854881,
      "grad_norm": 4.992344856262207,
      "learning_rate": 2.125140924464487e-05,
      "loss": 0.6588,
      "step": 1520
    },
    {
      "epoch": 2.0184696569920844,
      "grad_norm": 15.120711326599121,
      "learning_rate": 2.0969560315670803e-05,
      "loss": 0.6069,
      "step": 1530
    },
    {
      "epoch": 2.0316622691292876,
      "grad_norm": 7.610145092010498,
      "learning_rate": 2.0687711386696733e-05,
      "loss": 0.4853,
      "step": 1540
    },
    {
      "epoch": 2.044854881266491,
      "grad_norm": 9.246999740600586,
      "learning_rate": 2.040586245772266e-05,
      "loss": 0.546,
      "step": 1550
    },
    {
      "epoch": 2.058047493403694,
      "grad_norm": 5.905755519866943,
      "learning_rate": 2.0124013528748592e-05,
      "loss": 0.5305,
      "step": 1560
    },
    {
      "epoch": 2.0712401055408973,
      "grad_norm": 4.606659889221191,
      "learning_rate": 1.9842164599774522e-05,
      "loss": 0.5637,
      "step": 1570
    },
    {
      "epoch": 2.0844327176781,
      "grad_norm": 9.44939136505127,
      "learning_rate": 1.9560315670800452e-05,
      "loss": 0.5173,
      "step": 1580
    },
    {
      "epoch": 2.0976253298153034,
      "grad_norm": 11.286049842834473,
      "learning_rate": 1.9278466741826382e-05,
      "loss": 0.4532,
      "step": 1590
    },
    {
      "epoch": 2.1108179419525066,
      "grad_norm": 8.866724967956543,
      "learning_rate": 1.8996617812852312e-05,
      "loss": 0.4472,
      "step": 1600
    },
    {
      "epoch": 2.12401055408971,
      "grad_norm": 9.519842147827148,
      "learning_rate": 1.8714768883878242e-05,
      "loss": 0.5216,
      "step": 1610
    },
    {
      "epoch": 2.137203166226913,
      "grad_norm": 8.063298225402832,
      "learning_rate": 1.843291995490417e-05,
      "loss": 0.4231,
      "step": 1620
    },
    {
      "epoch": 2.150395778364116,
      "grad_norm": 9.34193229675293,
      "learning_rate": 1.81510710259301e-05,
      "loss": 0.4861,
      "step": 1630
    },
    {
      "epoch": 2.163588390501319,
      "grad_norm": 7.6244215965271,
      "learning_rate": 1.786922209695603e-05,
      "loss": 0.5147,
      "step": 1640
    },
    {
      "epoch": 2.1767810026385224,
      "grad_norm": 5.327208042144775,
      "learning_rate": 1.7587373167981965e-05,
      "loss": 0.547,
      "step": 1650
    },
    {
      "epoch": 2.1899736147757256,
      "grad_norm": 13.033745765686035,
      "learning_rate": 1.730552423900789e-05,
      "loss": 0.431,
      "step": 1660
    },
    {
      "epoch": 2.203166226912929,
      "grad_norm": 3.925886392593384,
      "learning_rate": 1.7023675310033824e-05,
      "loss": 0.5143,
      "step": 1670
    },
    {
      "epoch": 2.216358839050132,
      "grad_norm": 7.6628642082214355,
      "learning_rate": 1.6741826381059754e-05,
      "loss": 0.3755,
      "step": 1680
    },
    {
      "epoch": 2.229551451187335,
      "grad_norm": 5.436814308166504,
      "learning_rate": 1.6459977452085684e-05,
      "loss": 0.5945,
      "step": 1690
    },
    {
      "epoch": 2.242744063324538,
      "grad_norm": 7.371190547943115,
      "learning_rate": 1.6178128523111614e-05,
      "loss": 0.4089,
      "step": 1700
    },
    {
      "epoch": 2.2559366754617414,
      "grad_norm": 7.7575788497924805,
      "learning_rate": 1.589627959413754e-05,
      "loss": 0.5267,
      "step": 1710
    },
    {
      "epoch": 2.2691292875989446,
      "grad_norm": 5.040668964385986,
      "learning_rate": 1.5614430665163474e-05,
      "loss": 0.5252,
      "step": 1720
    },
    {
      "epoch": 2.282321899736148,
      "grad_norm": 23.148611068725586,
      "learning_rate": 1.5332581736189404e-05,
      "loss": 0.5483,
      "step": 1730
    },
    {
      "epoch": 2.295514511873351,
      "grad_norm": 10.869485855102539,
      "learning_rate": 1.5050732807215333e-05,
      "loss": 0.4901,
      "step": 1740
    },
    {
      "epoch": 2.308707124010554,
      "grad_norm": 10.022981643676758,
      "learning_rate": 1.4768883878241263e-05,
      "loss": 0.5748,
      "step": 1750
    },
    {
      "epoch": 2.321899736147757,
      "grad_norm": 9.6997652053833,
      "learning_rate": 1.4487034949267195e-05,
      "loss": 0.4527,
      "step": 1760
    },
    {
      "epoch": 2.3350923482849604,
      "grad_norm": 9.520753860473633,
      "learning_rate": 1.4205186020293123e-05,
      "loss": 0.446,
      "step": 1770
    },
    {
      "epoch": 2.3482849604221636,
      "grad_norm": 8.989245414733887,
      "learning_rate": 1.3923337091319053e-05,
      "loss": 0.4953,
      "step": 1780
    },
    {
      "epoch": 2.361477572559367,
      "grad_norm": 16.422033309936523,
      "learning_rate": 1.3641488162344984e-05,
      "loss": 0.5989,
      "step": 1790
    },
    {
      "epoch": 2.37467018469657,
      "grad_norm": 15.499666213989258,
      "learning_rate": 1.3359639233370913e-05,
      "loss": 0.5434,
      "step": 1800
    },
    {
      "epoch": 2.387862796833773,
      "grad_norm": 6.787818431854248,
      "learning_rate": 1.3077790304396844e-05,
      "loss": 0.5262,
      "step": 1810
    },
    {
      "epoch": 2.401055408970976,
      "grad_norm": 5.953798294067383,
      "learning_rate": 1.2795941375422774e-05,
      "loss": 0.3841,
      "step": 1820
    },
    {
      "epoch": 2.4142480211081794,
      "grad_norm": 7.458622932434082,
      "learning_rate": 1.2514092446448706e-05,
      "loss": 0.6675,
      "step": 1830
    },
    {
      "epoch": 2.4274406332453826,
      "grad_norm": 9.975418090820312,
      "learning_rate": 1.2232243517474634e-05,
      "loss": 0.5016,
      "step": 1840
    },
    {
      "epoch": 2.440633245382586,
      "grad_norm": 5.045845985412598,
      "learning_rate": 1.1950394588500564e-05,
      "loss": 0.4463,
      "step": 1850
    },
    {
      "epoch": 2.453825857519789,
      "grad_norm": 12.901169776916504,
      "learning_rate": 1.1668545659526495e-05,
      "loss": 0.4689,
      "step": 1860
    },
    {
      "epoch": 2.467018469656992,
      "grad_norm": 16.486709594726562,
      "learning_rate": 1.1386696730552425e-05,
      "loss": 0.4949,
      "step": 1870
    },
    {
      "epoch": 2.480211081794195,
      "grad_norm": 12.181427955627441,
      "learning_rate": 1.1104847801578353e-05,
      "loss": 0.4815,
      "step": 1880
    },
    {
      "epoch": 2.4934036939313984,
      "grad_norm": 5.86196231842041,
      "learning_rate": 1.0822998872604285e-05,
      "loss": 0.419,
      "step": 1890
    },
    {
      "epoch": 2.5065963060686016,
      "grad_norm": 14.98235034942627,
      "learning_rate": 1.0541149943630215e-05,
      "loss": 0.3524,
      "step": 1900
    },
    {
      "epoch": 2.519788918205805,
      "grad_norm": 18.243009567260742,
      "learning_rate": 1.0259301014656145e-05,
      "loss": 0.5483,
      "step": 1910
    },
    {
      "epoch": 2.532981530343008,
      "grad_norm": 9.407722473144531,
      "learning_rate": 9.977452085682074e-06,
      "loss": 0.424,
      "step": 1920
    },
    {
      "epoch": 2.5461741424802113,
      "grad_norm": 3.5308544635772705,
      "learning_rate": 9.695603156708006e-06,
      "loss": 0.5765,
      "step": 1930
    },
    {
      "epoch": 2.559366754617414,
      "grad_norm": 15.37695598602295,
      "learning_rate": 9.413754227733936e-06,
      "loss": 0.533,
      "step": 1940
    },
    {
      "epoch": 2.5725593667546174,
      "grad_norm": 14.303669929504395,
      "learning_rate": 9.131905298759866e-06,
      "loss": 0.4839,
      "step": 1950
    },
    {
      "epoch": 2.5857519788918206,
      "grad_norm": 16.570207595825195,
      "learning_rate": 8.850056369785794e-06,
      "loss": 0.5112,
      "step": 1960
    },
    {
      "epoch": 2.598944591029024,
      "grad_norm": 7.325024127960205,
      "learning_rate": 8.568207440811726e-06,
      "loss": 0.4322,
      "step": 1970
    },
    {
      "epoch": 2.6121372031662267,
      "grad_norm": 13.721790313720703,
      "learning_rate": 8.286358511837655e-06,
      "loss": 0.4926,
      "step": 1980
    },
    {
      "epoch": 2.62532981530343,
      "grad_norm": 12.750836372375488,
      "learning_rate": 8.004509582863585e-06,
      "loss": 0.6233,
      "step": 1990
    },
    {
      "epoch": 2.638522427440633,
      "grad_norm": 17.75196647644043,
      "learning_rate": 7.722660653889515e-06,
      "loss": 0.4196,
      "step": 2000
    },
    {
      "epoch": 2.6517150395778364,
      "grad_norm": 6.135998249053955,
      "learning_rate": 7.440811724915446e-06,
      "loss": 0.4413,
      "step": 2010
    },
    {
      "epoch": 2.6649076517150396,
      "grad_norm": 13.897343635559082,
      "learning_rate": 7.1589627959413766e-06,
      "loss": 0.5374,
      "step": 2020
    },
    {
      "epoch": 2.678100263852243,
      "grad_norm": 8.883676528930664,
      "learning_rate": 6.8771138669673056e-06,
      "loss": 0.4297,
      "step": 2030
    },
    {
      "epoch": 2.691292875989446,
      "grad_norm": 12.31254768371582,
      "learning_rate": 6.5952649379932354e-06,
      "loss": 0.5062,
      "step": 2040
    },
    {
      "epoch": 2.7044854881266494,
      "grad_norm": 4.3218994140625,
      "learning_rate": 6.313416009019166e-06,
      "loss": 0.4967,
      "step": 2050
    },
    {
      "epoch": 2.717678100263852,
      "grad_norm": 9.837594985961914,
      "learning_rate": 6.031567080045096e-06,
      "loss": 0.5688,
      "step": 2060
    },
    {
      "epoch": 2.7308707124010554,
      "grad_norm": 20.596893310546875,
      "learning_rate": 5.749718151071026e-06,
      "loss": 0.4398,
      "step": 2070
    },
    {
      "epoch": 2.7440633245382586,
      "grad_norm": 7.1950459480285645,
      "learning_rate": 5.467869222096957e-06,
      "loss": 0.4263,
      "step": 2080
    },
    {
      "epoch": 2.757255936675462,
      "grad_norm": 3.6247951984405518,
      "learning_rate": 5.1860202931228865e-06,
      "loss": 0.3985,
      "step": 2090
    },
    {
      "epoch": 2.7704485488126647,
      "grad_norm": 17.021886825561523,
      "learning_rate": 4.904171364148816e-06,
      "loss": 0.4658,
      "step": 2100
    },
    {
      "epoch": 2.783641160949868,
      "grad_norm": 5.092072010040283,
      "learning_rate": 4.622322435174747e-06,
      "loss": 0.483,
      "step": 2110
    },
    {
      "epoch": 2.796833773087071,
      "grad_norm": 6.221334934234619,
      "learning_rate": 4.340473506200677e-06,
      "loss": 0.4419,
      "step": 2120
    },
    {
      "epoch": 2.8100263852242744,
      "grad_norm": 9.38305377960205,
      "learning_rate": 4.058624577226607e-06,
      "loss": 0.3966,
      "step": 2130
    },
    {
      "epoch": 2.8232189973614776,
      "grad_norm": 4.629864692687988,
      "learning_rate": 3.7767756482525367e-06,
      "loss": 0.4799,
      "step": 2140
    },
    {
      "epoch": 2.836411609498681,
      "grad_norm": 12.091999053955078,
      "learning_rate": 3.494926719278467e-06,
      "loss": 0.4162,
      "step": 2150
    },
    {
      "epoch": 2.849604221635884,
      "grad_norm": 8.68013858795166,
      "learning_rate": 3.213077790304397e-06,
      "loss": 0.4353,
      "step": 2160
    },
    {
      "epoch": 2.862796833773087,
      "grad_norm": 6.496735095977783,
      "learning_rate": 2.931228861330327e-06,
      "loss": 0.4101,
      "step": 2170
    },
    {
      "epoch": 2.87598944591029,
      "grad_norm": 6.959737300872803,
      "learning_rate": 2.649379932356257e-06,
      "loss": 0.4681,
      "step": 2180
    },
    {
      "epoch": 2.8891820580474934,
      "grad_norm": 15.204031944274902,
      "learning_rate": 2.3675310033821873e-06,
      "loss": 0.4918,
      "step": 2190
    },
    {
      "epoch": 2.9023746701846966,
      "grad_norm": 6.655591011047363,
      "learning_rate": 2.0856820744081176e-06,
      "loss": 0.3576,
      "step": 2200
    },
    {
      "epoch": 2.9155672823219,
      "grad_norm": 4.378501892089844,
      "learning_rate": 1.8038331454340473e-06,
      "loss": 0.402,
      "step": 2210
    },
    {
      "epoch": 2.9287598944591027,
      "grad_norm": 8.307584762573242,
      "learning_rate": 1.5219842164599776e-06,
      "loss": 0.3827,
      "step": 2220
    },
    {
      "epoch": 2.941952506596306,
      "grad_norm": 7.417959213256836,
      "learning_rate": 1.2401352874859076e-06,
      "loss": 0.5576,
      "step": 2230
    },
    {
      "epoch": 2.955145118733509,
      "grad_norm": 11.865864753723145,
      "learning_rate": 9.582863585118377e-07,
      "loss": 0.482,
      "step": 2240
    },
    {
      "epoch": 2.9683377308707124,
      "grad_norm": 9.401460647583008,
      "learning_rate": 6.764374295377678e-07,
      "loss": 0.5488,
      "step": 2250
    },
    {
      "epoch": 2.9815303430079156,
      "grad_norm": 6.441432476043701,
      "learning_rate": 3.945885005636979e-07,
      "loss": 0.3984,
      "step": 2260
    },
    {
      "epoch": 2.994722955145119,
      "grad_norm": 18.37436294555664,
      "learning_rate": 1.1273957158962795e-07,
      "loss": 0.4987,
      "step": 2270
    }
  ],
  "logging_steps": 10,
  "max_steps": 2274,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2391700967147520.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
