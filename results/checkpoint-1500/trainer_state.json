{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.978891820580475,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013192612137203167,
      "grad_norm": 4.774919033050537,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.1185,
      "step": 10
    },
    {
      "epoch": 0.026385224274406333,
      "grad_norm": 4.0481414794921875,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.1164,
      "step": 20
    },
    {
      "epoch": 0.0395778364116095,
      "grad_norm": 6.396896839141846,
      "learning_rate": 3e-06,
      "loss": 1.1605,
      "step": 30
    },
    {
      "epoch": 0.052770448548812667,
      "grad_norm": 4.164666652679443,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.113,
      "step": 40
    },
    {
      "epoch": 0.06596306068601583,
      "grad_norm": 4.952052593231201,
      "learning_rate": 5e-06,
      "loss": 1.1276,
      "step": 50
    },
    {
      "epoch": 0.079155672823219,
      "grad_norm": 2.9794704914093018,
      "learning_rate": 6e-06,
      "loss": 1.1003,
      "step": 60
    },
    {
      "epoch": 0.09234828496042216,
      "grad_norm": 3.698364019393921,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.1296,
      "step": 70
    },
    {
      "epoch": 0.10554089709762533,
      "grad_norm": 5.2654290199279785,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.1304,
      "step": 80
    },
    {
      "epoch": 0.11873350923482849,
      "grad_norm": 6.673818588256836,
      "learning_rate": 9e-06,
      "loss": 1.1144,
      "step": 90
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 4.961310386657715,
      "learning_rate": 1e-05,
      "loss": 1.0906,
      "step": 100
    },
    {
      "epoch": 0.14511873350923482,
      "grad_norm": 6.018001556396484,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.0792,
      "step": 110
    },
    {
      "epoch": 0.158311345646438,
      "grad_norm": 8.471850395202637,
      "learning_rate": 1.2e-05,
      "loss": 1.1028,
      "step": 120
    },
    {
      "epoch": 0.17150395778364116,
      "grad_norm": 6.7678728103637695,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 1.105,
      "step": 130
    },
    {
      "epoch": 0.18469656992084432,
      "grad_norm": 4.957172870635986,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.0928,
      "step": 140
    },
    {
      "epoch": 0.19788918205804748,
      "grad_norm": 5.170859336853027,
      "learning_rate": 1.5e-05,
      "loss": 1.1274,
      "step": 150
    },
    {
      "epoch": 0.21108179419525067,
      "grad_norm": 5.3827362060546875,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.0753,
      "step": 160
    },
    {
      "epoch": 0.22427440633245382,
      "grad_norm": 4.848232746124268,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 1.1123,
      "step": 170
    },
    {
      "epoch": 0.23746701846965698,
      "grad_norm": 2.6167654991149902,
      "learning_rate": 1.8e-05,
      "loss": 1.11,
      "step": 180
    },
    {
      "epoch": 0.25065963060686014,
      "grad_norm": 3.279914140701294,
      "learning_rate": 1.9e-05,
      "loss": 1.0629,
      "step": 190
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 3.0368456840515137,
      "learning_rate": 2e-05,
      "loss": 1.0691,
      "step": 200
    },
    {
      "epoch": 0.2770448548812665,
      "grad_norm": 3.081223726272583,
      "learning_rate": 2.1e-05,
      "loss": 1.0582,
      "step": 210
    },
    {
      "epoch": 0.29023746701846964,
      "grad_norm": 6.3455424308776855,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.0174,
      "step": 220
    },
    {
      "epoch": 0.3034300791556728,
      "grad_norm": 5.767125606536865,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.9924,
      "step": 230
    },
    {
      "epoch": 0.316622691292876,
      "grad_norm": 4.938936233520508,
      "learning_rate": 2.4e-05,
      "loss": 1.0158,
      "step": 240
    },
    {
      "epoch": 0.32981530343007914,
      "grad_norm": 6.259737968444824,
      "learning_rate": 2.5e-05,
      "loss": 0.9764,
      "step": 250
    },
    {
      "epoch": 0.34300791556728233,
      "grad_norm": 6.126026630401611,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.99,
      "step": 260
    },
    {
      "epoch": 0.3562005277044855,
      "grad_norm": 3.945953845977783,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.0081,
      "step": 270
    },
    {
      "epoch": 0.36939313984168864,
      "grad_norm": 8.928479194641113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9792,
      "step": 280
    },
    {
      "epoch": 0.38258575197889183,
      "grad_norm": 6.918982028961182,
      "learning_rate": 2.9e-05,
      "loss": 0.9981,
      "step": 290
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 4.9141459465026855,
      "learning_rate": 3e-05,
      "loss": 0.9905,
      "step": 300
    },
    {
      "epoch": 0.40897097625329815,
      "grad_norm": 6.624162673950195,
      "learning_rate": 3.1e-05,
      "loss": 0.9372,
      "step": 310
    },
    {
      "epoch": 0.42216358839050133,
      "grad_norm": 6.654886245727539,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.0434,
      "step": 320
    },
    {
      "epoch": 0.43535620052770446,
      "grad_norm": 5.468522071838379,
      "learning_rate": 3.3e-05,
      "loss": 0.9974,
      "step": 330
    },
    {
      "epoch": 0.44854881266490765,
      "grad_norm": 4.315779209136963,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.949,
      "step": 340
    },
    {
      "epoch": 0.46174142480211083,
      "grad_norm": 6.919183254241943,
      "learning_rate": 3.5e-05,
      "loss": 1.0218,
      "step": 350
    },
    {
      "epoch": 0.47493403693931396,
      "grad_norm": 6.891890525817871,
      "learning_rate": 3.6e-05,
      "loss": 0.9498,
      "step": 360
    },
    {
      "epoch": 0.48812664907651715,
      "grad_norm": 6.477381229400635,
      "learning_rate": 3.7e-05,
      "loss": 0.9421,
      "step": 370
    },
    {
      "epoch": 0.5013192612137203,
      "grad_norm": 6.066005706787109,
      "learning_rate": 3.8e-05,
      "loss": 0.975,
      "step": 380
    },
    {
      "epoch": 0.5145118733509235,
      "grad_norm": 8.230584144592285,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.913,
      "step": 390
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 7.7161970138549805,
      "learning_rate": 4e-05,
      "loss": 0.9692,
      "step": 400
    },
    {
      "epoch": 0.5408970976253298,
      "grad_norm": 8.612902641296387,
      "learning_rate": 4.1e-05,
      "loss": 0.973,
      "step": 410
    },
    {
      "epoch": 0.554089709762533,
      "grad_norm": 9.606378555297852,
      "learning_rate": 4.2e-05,
      "loss": 0.9471,
      "step": 420
    },
    {
      "epoch": 0.5672823218997362,
      "grad_norm": 5.748153209686279,
      "learning_rate": 4.3e-05,
      "loss": 0.911,
      "step": 430
    },
    {
      "epoch": 0.5804749340369393,
      "grad_norm": 4.090378761291504,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.9028,
      "step": 440
    },
    {
      "epoch": 0.5936675461741425,
      "grad_norm": 6.769626617431641,
      "learning_rate": 4.5e-05,
      "loss": 0.8971,
      "step": 450
    },
    {
      "epoch": 0.6068601583113457,
      "grad_norm": 7.566318988800049,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.9425,
      "step": 460
    },
    {
      "epoch": 0.6200527704485488,
      "grad_norm": 10.706098556518555,
      "learning_rate": 4.7e-05,
      "loss": 0.9261,
      "step": 470
    },
    {
      "epoch": 0.633245382585752,
      "grad_norm": 5.926129341125488,
      "learning_rate": 4.8e-05,
      "loss": 0.9679,
      "step": 480
    },
    {
      "epoch": 0.6464379947229552,
      "grad_norm": 8.448880195617676,
      "learning_rate": 4.9e-05,
      "loss": 0.9818,
      "step": 490
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 5.845765590667725,
      "learning_rate": 5e-05,
      "loss": 0.9558,
      "step": 500
    },
    {
      "epoch": 0.6728232189973615,
      "grad_norm": 6.446487903594971,
      "learning_rate": 4.971815107102593e-05,
      "loss": 0.9924,
      "step": 510
    },
    {
      "epoch": 0.6860158311345647,
      "grad_norm": 7.992798805236816,
      "learning_rate": 4.943630214205186e-05,
      "loss": 0.8883,
      "step": 520
    },
    {
      "epoch": 0.6992084432717678,
      "grad_norm": 7.444221496582031,
      "learning_rate": 4.9154453213077795e-05,
      "loss": 1.0339,
      "step": 530
    },
    {
      "epoch": 0.712401055408971,
      "grad_norm": 8.335979461669922,
      "learning_rate": 4.887260428410372e-05,
      "loss": 0.9331,
      "step": 540
    },
    {
      "epoch": 0.7255936675461742,
      "grad_norm": 6.685624122619629,
      "learning_rate": 4.8590755355129655e-05,
      "loss": 0.8802,
      "step": 550
    },
    {
      "epoch": 0.7387862796833773,
      "grad_norm": 4.69497537612915,
      "learning_rate": 4.830890642615558e-05,
      "loss": 0.9464,
      "step": 560
    },
    {
      "epoch": 0.7519788918205804,
      "grad_norm": 6.663929462432861,
      "learning_rate": 4.8027057497181515e-05,
      "loss": 0.9533,
      "step": 570
    },
    {
      "epoch": 0.7651715039577837,
      "grad_norm": 6.173933982849121,
      "learning_rate": 4.774520856820744e-05,
      "loss": 0.8928,
      "step": 580
    },
    {
      "epoch": 0.7783641160949868,
      "grad_norm": 8.124838829040527,
      "learning_rate": 4.7463359639233375e-05,
      "loss": 0.9052,
      "step": 590
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 8.628262519836426,
      "learning_rate": 4.71815107102593e-05,
      "loss": 0.8878,
      "step": 600
    },
    {
      "epoch": 0.8047493403693932,
      "grad_norm": 10.564031600952148,
      "learning_rate": 4.6899661781285234e-05,
      "loss": 0.9149,
      "step": 610
    },
    {
      "epoch": 0.8179419525065963,
      "grad_norm": 10.276151657104492,
      "learning_rate": 4.661781285231117e-05,
      "loss": 0.8828,
      "step": 620
    },
    {
      "epoch": 0.8311345646437994,
      "grad_norm": 9.93553638458252,
      "learning_rate": 4.6335963923337094e-05,
      "loss": 0.8724,
      "step": 630
    },
    {
      "epoch": 0.8443271767810027,
      "grad_norm": 11.441752433776855,
      "learning_rate": 4.605411499436302e-05,
      "loss": 0.8671,
      "step": 640
    },
    {
      "epoch": 0.8575197889182058,
      "grad_norm": 5.396047592163086,
      "learning_rate": 4.5772266065388954e-05,
      "loss": 0.9669,
      "step": 650
    },
    {
      "epoch": 0.8707124010554089,
      "grad_norm": 4.161830425262451,
      "learning_rate": 4.549041713641488e-05,
      "loss": 0.8604,
      "step": 660
    },
    {
      "epoch": 0.8839050131926122,
      "grad_norm": 7.879155158996582,
      "learning_rate": 4.5208568207440814e-05,
      "loss": 0.9596,
      "step": 670
    },
    {
      "epoch": 0.8970976253298153,
      "grad_norm": 5.262300968170166,
      "learning_rate": 4.492671927846675e-05,
      "loss": 0.9154,
      "step": 680
    },
    {
      "epoch": 0.9102902374670184,
      "grad_norm": 7.873614311218262,
      "learning_rate": 4.464487034949267e-05,
      "loss": 0.9494,
      "step": 690
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 11.614933013916016,
      "learning_rate": 4.43630214205186e-05,
      "loss": 0.9578,
      "step": 700
    },
    {
      "epoch": 0.9366754617414248,
      "grad_norm": 3.8953700065612793,
      "learning_rate": 4.408117249154454e-05,
      "loss": 0.9574,
      "step": 710
    },
    {
      "epoch": 0.9498680738786279,
      "grad_norm": 7.75072717666626,
      "learning_rate": 4.3799323562570466e-05,
      "loss": 0.9168,
      "step": 720
    },
    {
      "epoch": 0.9630606860158312,
      "grad_norm": 6.092296123504639,
      "learning_rate": 4.351747463359639e-05,
      "loss": 0.8972,
      "step": 730
    },
    {
      "epoch": 0.9762532981530343,
      "grad_norm": 5.517991542816162,
      "learning_rate": 4.3235625704622326e-05,
      "loss": 0.8938,
      "step": 740
    },
    {
      "epoch": 0.9894459102902374,
      "grad_norm": 7.129178047180176,
      "learning_rate": 4.295377677564825e-05,
      "loss": 0.9601,
      "step": 750
    },
    {
      "epoch": 1.0026385224274406,
      "grad_norm": 6.4660725593566895,
      "learning_rate": 4.2671927846674186e-05,
      "loss": 0.9424,
      "step": 760
    },
    {
      "epoch": 1.0158311345646438,
      "grad_norm": 5.839738368988037,
      "learning_rate": 4.239007891770011e-05,
      "loss": 0.7598,
      "step": 770
    },
    {
      "epoch": 1.029023746701847,
      "grad_norm": 11.204663276672363,
      "learning_rate": 4.2108229988726046e-05,
      "loss": 0.7973,
      "step": 780
    },
    {
      "epoch": 1.04221635883905,
      "grad_norm": 13.899859428405762,
      "learning_rate": 4.182638105975197e-05,
      "loss": 0.7154,
      "step": 790
    },
    {
      "epoch": 1.0554089709762533,
      "grad_norm": 4.370832920074463,
      "learning_rate": 4.1544532130777905e-05,
      "loss": 0.8394,
      "step": 800
    },
    {
      "epoch": 1.0686015831134565,
      "grad_norm": 8.75367259979248,
      "learning_rate": 4.126268320180384e-05,
      "loss": 0.7298,
      "step": 810
    },
    {
      "epoch": 1.0817941952506596,
      "grad_norm": 13.466239929199219,
      "learning_rate": 4.0980834272829765e-05,
      "loss": 0.693,
      "step": 820
    },
    {
      "epoch": 1.0949868073878628,
      "grad_norm": 8.089147567749023,
      "learning_rate": 4.069898534385569e-05,
      "loss": 0.7598,
      "step": 830
    },
    {
      "epoch": 1.108179419525066,
      "grad_norm": 9.85766887664795,
      "learning_rate": 4.0417136414881625e-05,
      "loss": 0.7899,
      "step": 840
    },
    {
      "epoch": 1.121372031662269,
      "grad_norm": 8.501721382141113,
      "learning_rate": 4.013528748590756e-05,
      "loss": 0.7725,
      "step": 850
    },
    {
      "epoch": 1.1345646437994723,
      "grad_norm": 4.6986565589904785,
      "learning_rate": 3.9853438556933485e-05,
      "loss": 0.7152,
      "step": 860
    },
    {
      "epoch": 1.1477572559366755,
      "grad_norm": 12.170966148376465,
      "learning_rate": 3.957158962795942e-05,
      "loss": 0.6843,
      "step": 870
    },
    {
      "epoch": 1.1609498680738786,
      "grad_norm": 13.197447776794434,
      "learning_rate": 3.9289740698985344e-05,
      "loss": 0.8539,
      "step": 880
    },
    {
      "epoch": 1.1741424802110818,
      "grad_norm": 10.160172462463379,
      "learning_rate": 3.900789177001127e-05,
      "loss": 0.7557,
      "step": 890
    },
    {
      "epoch": 1.187335092348285,
      "grad_norm": 18.181018829345703,
      "learning_rate": 3.872604284103721e-05,
      "loss": 0.6868,
      "step": 900
    },
    {
      "epoch": 1.200527704485488,
      "grad_norm": 7.7242751121521,
      "learning_rate": 3.844419391206314e-05,
      "loss": 0.8233,
      "step": 910
    },
    {
      "epoch": 1.2137203166226913,
      "grad_norm": 11.453360557556152,
      "learning_rate": 3.8162344983089064e-05,
      "loss": 0.7884,
      "step": 920
    },
    {
      "epoch": 1.2269129287598945,
      "grad_norm": 8.255464553833008,
      "learning_rate": 3.7880496054115e-05,
      "loss": 0.7526,
      "step": 930
    },
    {
      "epoch": 1.2401055408970976,
      "grad_norm": 5.805210113525391,
      "learning_rate": 3.759864712514093e-05,
      "loss": 0.781,
      "step": 940
    },
    {
      "epoch": 1.2532981530343008,
      "grad_norm": 4.730753421783447,
      "learning_rate": 3.731679819616686e-05,
      "loss": 0.752,
      "step": 950
    },
    {
      "epoch": 1.266490765171504,
      "grad_norm": 14.113203048706055,
      "learning_rate": 3.703494926719279e-05,
      "loss": 0.788,
      "step": 960
    },
    {
      "epoch": 1.279683377308707,
      "grad_norm": 11.156713485717773,
      "learning_rate": 3.6753100338218716e-05,
      "loss": 0.6835,
      "step": 970
    },
    {
      "epoch": 1.2928759894459103,
      "grad_norm": 6.789685249328613,
      "learning_rate": 3.647125140924464e-05,
      "loss": 0.8196,
      "step": 980
    },
    {
      "epoch": 1.3060686015831133,
      "grad_norm": 5.612956523895264,
      "learning_rate": 3.6189402480270576e-05,
      "loss": 0.8046,
      "step": 990
    },
    {
      "epoch": 1.3192612137203166,
      "grad_norm": 8.131961822509766,
      "learning_rate": 3.590755355129651e-05,
      "loss": 0.7727,
      "step": 1000
    },
    {
      "epoch": 1.3324538258575198,
      "grad_norm": 7.388738632202148,
      "learning_rate": 3.5625704622322436e-05,
      "loss": 0.8013,
      "step": 1010
    },
    {
      "epoch": 1.345646437994723,
      "grad_norm": 10.62366008758545,
      "learning_rate": 3.534385569334836e-05,
      "loss": 0.8025,
      "step": 1020
    },
    {
      "epoch": 1.358839050131926,
      "grad_norm": 6.736076831817627,
      "learning_rate": 3.5062006764374296e-05,
      "loss": 0.7658,
      "step": 1030
    },
    {
      "epoch": 1.3720316622691293,
      "grad_norm": 14.679655075073242,
      "learning_rate": 3.478015783540023e-05,
      "loss": 0.7765,
      "step": 1040
    },
    {
      "epoch": 1.3852242744063323,
      "grad_norm": 12.908125877380371,
      "learning_rate": 3.4498308906426155e-05,
      "loss": 0.7931,
      "step": 1050
    },
    {
      "epoch": 1.3984168865435356,
      "grad_norm": 5.549347400665283,
      "learning_rate": 3.421645997745209e-05,
      "loss": 0.826,
      "step": 1060
    },
    {
      "epoch": 1.4116094986807388,
      "grad_norm": 8.370938301086426,
      "learning_rate": 3.3934611048478015e-05,
      "loss": 0.7884,
      "step": 1070
    },
    {
      "epoch": 1.424802110817942,
      "grad_norm": 6.76110315322876,
      "learning_rate": 3.365276211950395e-05,
      "loss": 0.7371,
      "step": 1080
    },
    {
      "epoch": 1.437994722955145,
      "grad_norm": 10.686677932739258,
      "learning_rate": 3.337091319052988e-05,
      "loss": 0.7692,
      "step": 1090
    },
    {
      "epoch": 1.4511873350923483,
      "grad_norm": 9.013206481933594,
      "learning_rate": 3.308906426155581e-05,
      "loss": 0.8472,
      "step": 1100
    },
    {
      "epoch": 1.4643799472295513,
      "grad_norm": 8.006030082702637,
      "learning_rate": 3.2807215332581735e-05,
      "loss": 0.7936,
      "step": 1110
    },
    {
      "epoch": 1.4775725593667546,
      "grad_norm": 14.062169075012207,
      "learning_rate": 3.252536640360767e-05,
      "loss": 0.6907,
      "step": 1120
    },
    {
      "epoch": 1.4907651715039578,
      "grad_norm": 12.238818168640137,
      "learning_rate": 3.22435174746336e-05,
      "loss": 0.7271,
      "step": 1130
    },
    {
      "epoch": 1.503957783641161,
      "grad_norm": 4.583370208740234,
      "learning_rate": 3.196166854565953e-05,
      "loss": 0.8062,
      "step": 1140
    },
    {
      "epoch": 1.517150395778364,
      "grad_norm": 6.923147201538086,
      "learning_rate": 3.167981961668546e-05,
      "loss": 0.7033,
      "step": 1150
    },
    {
      "epoch": 1.5303430079155673,
      "grad_norm": 12.416332244873047,
      "learning_rate": 3.139797068771139e-05,
      "loss": 0.7674,
      "step": 1160
    },
    {
      "epoch": 1.5435356200527703,
      "grad_norm": 6.812841415405273,
      "learning_rate": 3.111612175873732e-05,
      "loss": 0.709,
      "step": 1170
    },
    {
      "epoch": 1.5567282321899736,
      "grad_norm": 12.708789825439453,
      "learning_rate": 3.083427282976325e-05,
      "loss": 0.7386,
      "step": 1180
    },
    {
      "epoch": 1.5699208443271768,
      "grad_norm": 6.587008476257324,
      "learning_rate": 3.055242390078918e-05,
      "loss": 0.7797,
      "step": 1190
    },
    {
      "epoch": 1.58311345646438,
      "grad_norm": 13.54847526550293,
      "learning_rate": 3.0270574971815107e-05,
      "loss": 0.746,
      "step": 1200
    },
    {
      "epoch": 1.596306068601583,
      "grad_norm": 5.945424556732178,
      "learning_rate": 2.9988726042841037e-05,
      "loss": 0.7024,
      "step": 1210
    },
    {
      "epoch": 1.6094986807387863,
      "grad_norm": 5.665104389190674,
      "learning_rate": 2.970687711386697e-05,
      "loss": 0.7996,
      "step": 1220
    },
    {
      "epoch": 1.6226912928759893,
      "grad_norm": 7.0391740798950195,
      "learning_rate": 2.94250281848929e-05,
      "loss": 0.757,
      "step": 1230
    },
    {
      "epoch": 1.6358839050131926,
      "grad_norm": 15.129524230957031,
      "learning_rate": 2.914317925591883e-05,
      "loss": 0.7721,
      "step": 1240
    },
    {
      "epoch": 1.6490765171503958,
      "grad_norm": 7.252932071685791,
      "learning_rate": 2.8861330326944756e-05,
      "loss": 0.774,
      "step": 1250
    },
    {
      "epoch": 1.662269129287599,
      "grad_norm": 8.50764274597168,
      "learning_rate": 2.8579481397970686e-05,
      "loss": 0.8067,
      "step": 1260
    },
    {
      "epoch": 1.675461741424802,
      "grad_norm": 6.990533351898193,
      "learning_rate": 2.829763246899662e-05,
      "loss": 0.7643,
      "step": 1270
    },
    {
      "epoch": 1.6886543535620053,
      "grad_norm": 13.39630126953125,
      "learning_rate": 2.801578354002255e-05,
      "loss": 0.8066,
      "step": 1280
    },
    {
      "epoch": 1.7018469656992083,
      "grad_norm": 10.75790786743164,
      "learning_rate": 2.773393461104848e-05,
      "loss": 0.8097,
      "step": 1290
    },
    {
      "epoch": 1.7150395778364116,
      "grad_norm": 8.505586624145508,
      "learning_rate": 2.745208568207441e-05,
      "loss": 0.6907,
      "step": 1300
    },
    {
      "epoch": 1.7282321899736148,
      "grad_norm": 11.688013076782227,
      "learning_rate": 2.7170236753100342e-05,
      "loss": 0.8379,
      "step": 1310
    },
    {
      "epoch": 1.741424802110818,
      "grad_norm": 4.244495868682861,
      "learning_rate": 2.6888387824126272e-05,
      "loss": 0.7032,
      "step": 1320
    },
    {
      "epoch": 1.754617414248021,
      "grad_norm": 5.4763617515563965,
      "learning_rate": 2.66065388951522e-05,
      "loss": 0.7035,
      "step": 1330
    },
    {
      "epoch": 1.767810026385224,
      "grad_norm": 9.331557273864746,
      "learning_rate": 2.632468996617813e-05,
      "loss": 0.7512,
      "step": 1340
    },
    {
      "epoch": 1.7810026385224274,
      "grad_norm": 7.604462623596191,
      "learning_rate": 2.604284103720406e-05,
      "loss": 0.7008,
      "step": 1350
    },
    {
      "epoch": 1.7941952506596306,
      "grad_norm": 13.975248336791992,
      "learning_rate": 2.576099210822999e-05,
      "loss": 0.7329,
      "step": 1360
    },
    {
      "epoch": 1.8073878627968338,
      "grad_norm": 8.140799522399902,
      "learning_rate": 2.547914317925592e-05,
      "loss": 0.7458,
      "step": 1370
    },
    {
      "epoch": 1.820580474934037,
      "grad_norm": 8.626213073730469,
      "learning_rate": 2.519729425028185e-05,
      "loss": 0.6874,
      "step": 1380
    },
    {
      "epoch": 1.83377308707124,
      "grad_norm": 3.844200611114502,
      "learning_rate": 2.491544532130778e-05,
      "loss": 0.7226,
      "step": 1390
    },
    {
      "epoch": 1.8469656992084431,
      "grad_norm": 9.6218843460083,
      "learning_rate": 2.463359639233371e-05,
      "loss": 0.7809,
      "step": 1400
    },
    {
      "epoch": 1.8601583113456464,
      "grad_norm": 7.94784688949585,
      "learning_rate": 2.435174746335964e-05,
      "loss": 0.8179,
      "step": 1410
    },
    {
      "epoch": 1.8733509234828496,
      "grad_norm": 10.888376235961914,
      "learning_rate": 2.406989853438557e-05,
      "loss": 0.6725,
      "step": 1420
    },
    {
      "epoch": 1.8865435356200528,
      "grad_norm": 4.975091457366943,
      "learning_rate": 2.37880496054115e-05,
      "loss": 0.7199,
      "step": 1430
    },
    {
      "epoch": 1.899736147757256,
      "grad_norm": 9.931329727172852,
      "learning_rate": 2.350620067643743e-05,
      "loss": 0.6571,
      "step": 1440
    },
    {
      "epoch": 1.912928759894459,
      "grad_norm": 9.12621021270752,
      "learning_rate": 2.322435174746336e-05,
      "loss": 0.6468,
      "step": 1450
    },
    {
      "epoch": 1.9261213720316621,
      "grad_norm": 9.160064697265625,
      "learning_rate": 2.2942502818489294e-05,
      "loss": 0.6989,
      "step": 1460
    },
    {
      "epoch": 1.9393139841688654,
      "grad_norm": 10.730857849121094,
      "learning_rate": 2.266065388951522e-05,
      "loss": 0.6603,
      "step": 1470
    },
    {
      "epoch": 1.9525065963060686,
      "grad_norm": 13.0791597366333,
      "learning_rate": 2.237880496054115e-05,
      "loss": 0.7734,
      "step": 1480
    },
    {
      "epoch": 1.9656992084432718,
      "grad_norm": 9.550217628479004,
      "learning_rate": 2.209695603156708e-05,
      "loss": 0.8576,
      "step": 1490
    },
    {
      "epoch": 1.978891820580475,
      "grad_norm": 8.91973876953125,
      "learning_rate": 2.181510710259301e-05,
      "loss": 0.6994,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 2274,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1578154279532544.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
